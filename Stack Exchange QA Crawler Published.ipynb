{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Exchange QA Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Permission Settings \n",
    "1. log in your stackexchange account first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your application information here\n",
    "\n",
    "client_id = '' # client_id\n",
    "client_secret = '' # client_secret\n",
    "key = '' # key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_oauthlib import OAuth2Session\n",
    "from pprint import pprint\n",
    "\n",
    "redirect_uri = 'https://stackexchange.com/oauth/login_success'\n",
    "scope = 'no_expiry'\n",
    "\n",
    "oauth = OAuth2Session(client_id, redirect_uri=redirect_uri, scope=scope)\n",
    "\n",
    "# pprint(vars(oauth))\n",
    "\n",
    "authorization_url, state = oauth.authorization_url('https://stackexchange.com/oauth/dialog')\n",
    "\n",
    "print(\"Please click in this link:\", authorization_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the link from the browser and put in the 'response_url' below\n",
    "print(\"copy the link from the browser and put in the 'response_url' below\")\n",
    "response_url = input()\n",
    "access_token = response_url.split('=')[1].split('&')[0]\n",
    "access_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Sentiment Analyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from azure.cognitiveservices.language.textanalytics import TextAnalyticsClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from pprint import pprint\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.client = self.authenticateClient()\n",
    "\n",
    "    # Creates client for sending requests to azure textanalytics api\n",
    "    def authenticateClient(self):\n",
    "        # TODO: change endpoint and key\n",
    "\n",
    "        endpoint = ''\n",
    "        key = ''\n",
    "\n",
    "        credentials = CognitiveServicesCredentials(key)\n",
    "        text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credentials=credentials)\n",
    "        return text_analytics_client\n",
    "\n",
    "    # creates the 'documentsList' which will be sent to azure, for a set of answers.\n",
    "    def createDocumentsList(self, answers):\n",
    "        count = 1\n",
    "        documentsList = []\n",
    "        for answer in answers:\n",
    "            count += 1\n",
    "            document = self.createDocument(answer, count)\n",
    "            documentsList.append(document)\n",
    "\n",
    "        return documentsList\n",
    "\n",
    "    # creates a document for one specific answer\n",
    "    def createDocument(self, answer, count):\n",
    "        body = self.removeMarkUps(answer[\"body\"])\n",
    "        # to get sentiment linked to answer_id use `str(answer[\"answer_id\"])`` instead of str(count)`\n",
    "        document = {\n",
    "            \"language\": \"en\",\n",
    "            \"id\": str(answer[\"answer_id\"]),\n",
    "            \"text\": body\n",
    "        }\n",
    "        return document\n",
    "\n",
    "    # max documentList size is 1000 elements/ids and documents can have no more than 5120 characters\n",
    "    # analyzes the sentiments of a list of documents (in our case answers), and returns the api response\n",
    "    def getSentimentAnalysis(self, documents):\n",
    "        response = self.client.sentiment(documents=documents)\n",
    "        return response\n",
    "\n",
    "    # analyzes one post and the corresponding answers. analyze means: \"what is the sentiment score of the answers?\"\n",
    "    # input:\n",
    "    #     the json of a stackexchange query, in the format s.t.:\n",
    "    #          one item in the items list is one question with all the answers in an answer list\n",
    "    # return: returns a list of posts with the corresponding sentiment analysis\n",
    "    def analyzeFullPostWithAnswers(self, batch):\n",
    "        multiplePostsWithAnalysis = []\n",
    "        for item in batch[\"items\"]:\n",
    "            if \"answers\" in item:\n",
    "                response = self.getSentimentAnalysis(\n",
    "                    self.createDocumentsList(item[\"answers\"]))\n",
    "                postWithAnalysis = {\n",
    "                    \"question_id\": item[\"question_id\"],\n",
    "                    \"user_id\": item[\"owner\"][\"user_id\"],\n",
    "                    \"response_with_scores\": self.azureResponseToJson(response)\n",
    "                }\n",
    "                multiplePostsWithAnalysis.append(postWithAnalysis)\n",
    "\n",
    "        return multiplePostsWithAnalysis\n",
    "\n",
    "    def analyzeFullAnswersWithAnswers(self, batch):\n",
    "        multiplePostsWithAnalysis = []\n",
    "        for item in batch[\"items\"]:\n",
    "            temp_list = []\n",
    "            temp_list.append(item)\n",
    "            response = self.getSentimentAnalysis(\n",
    "                self.createDocumentsList(temp_list))\n",
    "            postWithAnalysis = {\n",
    "                \"question_id\": item[\"question_id\"],\n",
    "                \"user_id\": item[\"owner\"][\"user_id\"],\n",
    "                \"response_with_scores\": self.azureResponseToJson(response)\n",
    "            }\n",
    "            multiplePostsWithAnalysis.append(postWithAnalysis)\n",
    "\n",
    "        return multiplePostsWithAnalysis\n",
    "\n",
    "    # Formats the azure api response to a json object and returns it\n",
    "    def azureResponseToJson(self, response):\n",
    "        answer_list = []\n",
    "        for document in response.documents:\n",
    "            answer = {\"answer_id\": document.id,\n",
    "                      \"sentiment_score\": \"{:.2f}\".format(document.score)}\n",
    "            answer_list.append(answer)\n",
    "\n",
    "        return answer_list\n",
    "\n",
    "    # helper function to remove code from text-/answer-body\n",
    "    def remove_code_from_posts(self, raw_html):\n",
    "        clean_up = re.sub(\"<code>.*?</code>\", \"\", raw_html)\n",
    "        return clean_up\n",
    "\n",
    "    # helper function to remove markup from text-/answer-body\n",
    "    def removeMarkUps(self, markUpText):\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        cleantext = re.sub(cleanr, '', self.remove_code_from_posts(markUpText))\n",
    "        return cleantext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Sentimental Analysis and Answer Crawler into Loop version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# please fill in the following parameter to use\n",
    "input_filename = \"user_data/user_male_sample.csv\"\n",
    "output_filename = \"answers_male_sample.json\"\n",
    "question_num_per_user = 50 # the maximum for question we need is 50\n",
    "users_num = 3 # for debug\n",
    "\n",
    "cleaned_users = pd.read_csv(input_filename)\n",
    "cleaned_users\n",
    "\n",
    "BASEURL = \"https://api.stackexchange.com/2.2/users/\"\n",
    "\n",
    "site = 'stackoverflow'\n",
    "page = 1\n",
    "pagesize = question_num_per_user\n",
    "order = 'desc'\n",
    "sort = 'activity'\n",
    "filter_param = '!5-dTNUfY0JWMhwHvF.hA6K4v8DSD0wes1D14dD'\n",
    "\n",
    "params = {\n",
    "    \"site\" : site,\n",
    "    \"page\" : page,\n",
    "    \"pagesize\": pagesize,\n",
    "    \"order\": order,\n",
    "    \"sort\": sort,\n",
    "    \"filter\": filter_param,\n",
    "    \"access_token\" : access_token,\n",
    "    \"key\" : key # change the key to your \n",
    "}\n",
    "\n",
    "userSentimentAnalysisResults={}\n",
    "\n",
    "for idx,row in cleaned_users.iterrows():\n",
    "    user_id = str(row['user_id'])\n",
    "    ADD_ID_URL = BASEURL + user_id + '/answers'\n",
    "    r = requests.get(ADD_ID_URL, params=params)\n",
    "    \n",
    "    sleep_time = 0\n",
    "    if 'backoff' in r.json().keys():\n",
    "        sleep_time = r.json()['backoff']\n",
    "        print('backoff:', sleep_time)\n",
    "    time.sleep(sleep_time)\n",
    "   \n",
    "    sa = SentimentAnalyzer()\n",
    "    analyzedWithSentiments = sa.analyzeFullAnswersWithAnswers(r.json())\n",
    "    userSentimentAnalysisResults[user_id] = analyzedWithSentiments\n",
    "    \n",
    "    with open(output_filename, 'w') as fp:\n",
    "        json.dump(userSentimentAnalysisResults, fp, indent=4)\n",
    "    \n",
    "    print(idx,row['user_id'],'quota_remaining', r.json()['quota_remaining'])\n",
    "#     print(r.json())\n",
    "#     print(analyzedWithSentiments)\n",
    "#     if users_num == 0: # for debug\n",
    "#         break          # for debug\n",
    "#     users_num-=1       # for debug \n",
    "\n",
    "# write to json\n",
    "# with open(output_filename, 'w') as fp:\n",
    "#     json.dump(userSentimentAnalysisResults, fp, indent=4)\n",
    "#         pprint(analyzedWithSentiments)\n",
    "#         pprint(r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(\"answers_male_sample_temp.json\", 'w') as fp:\n",
    "    json.dump(userSentimentAnalysisResults, fp, indent=4)\n",
    "userSentimentAnalysisResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(output_filename, 'r') as file: # change the filename\n",
    "    female_dict = json.load(file)\n",
    "\n",
    "    response_list = []\n",
    "    for k in female_dict:\n",
    "        if len(female_dict[k]) >= 1:\n",
    "            for el in female_dict[k]:\n",
    "                response = el\n",
    "                response_list.append(response)\n",
    "\n",
    "    # print(response_list[0]['response_with_scores'][0]['sentiment_score'])\n",
    "\n",
    "    sentiment_list = [] # saves all sentiment scores in a list\n",
    "    i = -1\n",
    "    for r in response_list:\n",
    "        i+= 1\n",
    "        if 'response_with_scores' in r:\n",
    "            if len(response_list[i]['response_with_scores']) >= 1: # if the value of 'response_with_scores' is an empty list\n",
    "                sentiment = response_list[i]['response_with_scores'][0]['sentiment_score']\n",
    "                sentiment_list.append(float(sentiment))\n",
    "\n",
    "    # print(sentiment_list)\n",
    "\n",
    "    total = 0\n",
    "    for score in sentiment_list:\n",
    "        total += score\n",
    "    \n",
    "    avg = total/len(sentiment_list)\n",
    "    print(\"average score: {}\".format(round(avg, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}